---
title: "Chapter13Teetor"
author: "Muhammed Khan"
date: "Thursday, April 16, 2015"
output: pdf_document
---
```{r}
#importing actual clickthrough data set to perform r commands and operations on

actualdata<-read.csv("C:/Users/AliDesktop/Desktop/Bit Briefcase/Big Data/Kaggle/CTR/train.csv",
                     nrow=1000)

#Checking variables of the clickthrough data set

names(actualdata)

#Minimizing or Maximizing a Single-Parameter Function
f <- function(x) 3*x^4 - 2*x^3 + 3*x^2 - 4*x + 5
optimize(f, lower=-20, upper=20)  #minimise

#Performing Principal Component Analysis-using prcomp function
r <- prcomp(~actualdata$C14+actualdata$C17+actualdata$C18+actualdata$C19)
summary(r)  #summary shows the proportion of variation captured by each component

#above summary shows that first component PC1 captures ~99.5% of the variance and other 
# components capturing remaining


#The PCA recasts data into a vector space where the 1st dimension captures the most variance
#and the second dimension captures the second most, and so forth.
print(r)

#in order to view a bar chart of the variances of the principal components, 
# use the below command
plot(r)

#in order to rotate our actualdata to the principal components, use the below command-
predict(r)


#-------------Performing Simple Orthogonal Regression-Also called as total least squares---------
#To create a linear model using orthogonal regression in which variances of C18 and C19 
# are treated symmetricallyin order to implement a basic orthogonal regression in R, 
# we perform PCA

r <- prcomp( ~ actualdata$C18 + actualdata$C19 )
#Now, using the rotations to compute the slope:
slope <- r$rotation[2,1] / r$rotation[1,1]
#Now, calculatng the intercept from the slope:
intercept <- r$center[2] - slope*r$center[1]


#------Finding Clusters in the Data---------------

#creating a subset of the actual clickthrough data set to include only numerical variables
# to understand clustering

#d<-dist(x)      #Compute distances between observations
#hc <- hclust(d)  #Form hierarchical clusters

#the result clust below is the vector of numbers between 1 and 3, one for each observation in x
#Each number classifies its corresponding observation into one of the n clusters.
#clust <- cutree(hc, k=3)   #Organize them into the 3 largest clusters

#-----------Predicting a Binary-Valued Variable (Logistic Regression)---------
#A regression model to predict the probability of a binary event occuring

install.packages("faraway")
library(faraway)

#Faraway gives an example of predicting a binary-valued variable: 
#test from the dataset pima is true if the patient tested positive for diabetes.

data(pima, package="faraway")
b <- factor(pima$test)

#The predictors are diastolic blood pressure and body mass index (BMI).
m <- glm(b ~ diastolic + bmi, family=binomial, data=pima) 

summary(m)   #results show that only the bmi variable is significant, p-value for it is 
# 1.95e-14 

#Since only bmi variable is significant, a reduced model can be created like below:
m.red <- glm(b ~ bmi, family=binomial, data=pima)


#Now using the model to calculate the probability that someone with an avg BMI(32.0) 
# will test positive for diabetes
newdata <- data.frame(bmi=32.0) 

predict(m.red, type="response", newdata=newdata)
#According to this model, the probability is about 33.3%

#-------Factor Analysis------

#in order to discover what the variables in a dataset have in common, we use the factanal
# function

#creating a subset of the actual clickthrough data set to include only numerical variables
# since factor analysis is for numerical ones

x<-data.frame(actualdata$C14, actualdata$C18, actualdata$C19, actualdata$C17, actualdata$C20,
              actualdata$C21)

#Plotting the PCA to see the variance captured by the components
plot(prcomp(x))

factanal(x,factors=2) #The p-value is 9.4e-12. Small p-value (<0.05) indicates that the two
# factors are insufficient

#In cases where p-value>0.05, it will help us to conclude that factors are sufficient 
#and % of individual variance and cumulative variance they explain
```

