{
    "contents" : "\n#getwd() \n#\"C:/Users/AliDesktop/Desktop/Magic Briefcase/Internship\"\n\n#loading required packages to ingest data\n#ingest split aggregate combine\nlibrary(plyr)\nlibrary(dplyr)\n\nfirst_time = FALSE\n\nif(first_time){\n  # function for data ingestion. this will load all the csvs into R as one data set\n  file_names = list.files(path=\"Internship/Jan Fridge Data\",pattern=\"csv\",full.names=TRUE)\n  mydata = ldply(file_names,function(filename) {\n    dum = read.csv(filename)\n    dum$filename=filename\n    return(dum)\n  })\n  save(mydata, file='data/all_files.RData')\n}\n\nload('data/all_files.RData')\n\n\n#check total numbers of rows and no of variables in the ingested data\ndim(mydata)\n\n### [1] 2602496       9\n\n\n#to get a summary stats of the ingested data\nsummary(mydata)\n \n############# \n\n## TemperatureID          SensorID     OriginalReading    ReadingDate         GroupID    \n## Min.   :1.089e+09   Min.   :23152   Min.   :-32.000   00:56.4:     31   Min.   : 811  \n## 1st Qu.:1.137e+09   1st Qu.:23479   1st Qu.:-17.100   00:55.9:     28   1st Qu.:1911  \n## Median :1.186e+09   Median :23816   Median :  3.200   00:56.2:     28   Median :2983  \n## Mean   :1.186e+09   Mean   :25332   Mean   : -1.735   15:56.1:     28   Mean   :3666  \n## 3rd Qu.:1.234e+09   3rd Qu.:24148   3rd Qu.:  5.100   30:56.3:     28   3rd Qu.:5634  \n## Max.   :1.320e+09   Max.   :84192   Max.   : 28.200   45:55.7:     28   Max.   :8639  \n##                                                       (Other):2602325                 \n##                                                                                                 Notes        \n## Type: RX Ambient,Make : (Ambient),Model: (Ambient),State: RI                                       :  91613  \n##                                                                                                    :  42057  \n## Type: RX Cooler,Make : Master-Bilt,Model: BGR-14R,State: RI Notes:  One clear door cooler back wall:  26149  \n## Type: RX Ambient,Make : ,Model: ,State: RI                                                         :  20550  \n## Type: RX Cooler,Make : haier,Model: n/a,State: RI                                                  :  17870  \n## (Other)                                                                                            :2283164  \n## NA's                                                                                               : 121093  \n##\n## \n## Make              Model          filename        \n## Make_2 :1022544   Model_1 :814616   Length:2602496    \n## Make_1 : 604678   Model_12:510929   Class :character  \n## Make_4 : 193506   Model_2 :178601   Mode  :character  \n## Make_10: 186090   Model_6 :131011                     \n## Make_6 : 125305   Model_9 :121315                     \n## Make_13: 103358   Model_22:116027                     \n## (Other): 367015   (Other) :729997       \n## \n##--------------\n##################\n\n\n#to get the column names of the data set \ncolnames(mydata)\n\n####[1] \"TemperatureID\"   \"SensorID\"        \"OriginalReading\" \"ReadingDate\"     \"GroupID\"         \"Notes\"          \n####[7] \"Make\"            \"Model\"           \"filename\"  \n\n\n######follow up #########\n\ntable(mydata$SensorID)\n\nas.data.frame(table(mydata$SensorID))\n####there are total 874 SensorIDs\n\nas.data.frame(table(mydata$GroupID))\n####there are total of 61 GroupIDs\n\n\n\n#########installing the required package###################\n\ninstall.packages(\"doBy\")\n\n## groupwise stats\n\n\nlibrary(doBy)\n\n####getting the mean and std. deviation value for each sensor\nsummarydata<-summaryBy(OriginalReading ~ SensorID, data = mydata, FUN = function(x) { c(m = mean(x), s = sd(x)) } )\nsummarydata<-data.frame(summarydata)\ncolnames(summarydata)\n\n#####calculating mean+3*s.d and mean-3*s.d for coming up with upper limit and lower limit\nsummarydata$upper_limit<-(summarydata$OriginalReading.m + 3 * summarydata$OriginalReading.s)\n\nsummarydata$lower_limit<-(summarydata$OriginalReading.m - 3 * summarydata$OriginalReading.s)\n\n##merging the statistics (mean, s.d, uper limit and lower limit to the actual data set#####\nalldata<-merge(x = mydata, y = summarydata, by = \"SensorID\", all.x = TRUE)\n\ncolnames(alldata)\n\nalldata$classify <- ifelse((alldata$OriginalReading > alldata$upper_limit)|(alldata$OriginalReading < alldata$lower_limit) ,1,0)\n\n#checks ...this is just to confirm if merge has happened correctly, no of observations with anamolies etc..\n\nnewdata <- alldata[which(alldata$classify==1),] \nnrow(newdata)\n##[1] 21204\n##there are total 21,204 observations in total with anamolies\n\ndim(newdata)\n##[1] 21204    14\n\nwrite.csv(newdata, file = \"C:\\\\data.csv\")\n\n####checks ends here\n\n######getting the top 10 sensors with most number of anamolies (i.e readings which is deviation from what is normal)\n\nrequire(doBy)\n####summing and grouping\n#######getting total no of anamolies for each sensor\naggdata<-summaryBy(classify~SensorID, data=alldata, FUN=sum)\n\n#########sorting by top sensors with anamolies\n\naggdata <- aggdata[order(-aggdata$classify),] \n\n###listing the top 10 sensors with most anamolies\nhead(aggdata,10)\n\n##   SensorID classify.sum\n##   24176          256\n##   82011          226\n##   42607          138\n##   23914          124\n##   63840          123\n##   63421          118\n##   23361          115\n##   24063          114\n##   23511          111\n##   23866          111\n\naggdata <- head(aggdata,10)\n\n#####looking at the distribution of these top 10 sensors#####################\n\n#getting subset of data which includes only the top 10 sensors\n#perform SQL selects on dataframes\n\ninstall.packages(\"sqldf\")\n\nlibrary(sqldf)\n\ndf <- sqldf(\"SELECT OriginalReading, ReadingDate, GroupID, SensorID, classify FROM alldata JOIN aggdata USING(SensorID)\")\n\n###top sensor 10 with anamoly\ndf1<-alldata[which(alldata$SensorID==23866), ]\nplot(df1$OriginalReading, main = \"Sensor1 id:23866\", xlab=\"observation#\", ylab=\"Original_Reading\")\n\n###top sensor 9 with anamoly\n\ndf2<-alldata[which(alldata$SensorID==23511), ]\nplot(df2$OriginalReading, main = \"Sensor2 id:23511\", xlab=\"observation#\", ylab=\"Original_Reading\")\n\n###top sensor 8 with anamoly\n\ndf3<-alldata[which(alldata$SensorID==24063), ]\nplot(df3$OriginalReading, main = \"Sensor3 id:24063\", xlab=\"observation#\", ylab=\"Original_Reading\")\n###top sensor 7 with anamoly\n\ndf4<-alldata[which(alldata$SensorID==23361), ]\nplot(df4$OriginalReading, main = \"Sensor4 id:23361\", xlab=\"observation#\", ylab=\"Original_Reading\")\n###top sensor 6 with anamoly\n\ndf5<-alldata[which(alldata$SensorID==63421), ]\nplot(df5$OriginalReading, main = \"Sensor5 id:63421\", xlab=\"observation#\", ylab=\"Original_Reading\")\n###top sensor 5 with anamoly\n\ndf6<-alldata[which(alldata$SensorID==63840), ]\nplot(df6$OriginalReading, main = \"Sensor6 id:63840\", xlab=\"observation#\", ylab=\"Original_Reading\")\n###top sensor 4 with anamoly\n\ndf7<-alldata[which(alldata$SensorID==23914), ]\nplot(df7$OriginalReading, main = \"Sensor7 id:23914\", xlab=\"observation#\", ylab=\"Original_Reading\")\n###top sensor 3 with anamoly\n\ndf8<-alldata[which(alldata$SensorID==42607), ]\nplot(df8$OriginalReading, main = \"Sensor8 id:42607\", xlab=\"observation#\", ylab=\"Original_Reading\")\n###top sensor 2 with anamoly\n\ndf9<-alldata[which(alldata$SensorID==82011), ]\nplot(df9$OriginalReading, main = \"Sensor9 id:82011\", xlab=\"observation#\", ylab=\"Original_Reading\")\n###top sensor 1 with anamoly\n\ndf10<-alldata[which(alldata$SensorID==24176), ]\nplot(df10$OriginalReading, main = \"Sensor10 id:24176\", xlab=\"observation#\", ylab=\"Original_Reading\")\n\n\n",
    "created" : 1458594387719.000,
    "dirty" : false,
    "encoding" : "LATIN1",
    "folds" : "",
    "hash" : "3208609989",
    "id" : "FA8940DD",
    "lastKnownWriteTime" : 1458666433,
    "path" : "~/Desktop/dm/freelance/03_march/ml_sensor/data_ingestion_r_0302.r",
    "project_path" : "data_ingestion_r_0302.r",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_source"
}