This is sensor data for a cooling/refrigerator unit.

This was a wrong analysis according to engineer I work with. Reason is in image dfx1,2,3.jpeg you can that the bottom temperature is the device temperature data and the top is the defrost temperature. I took the mean and omitted 3 standard deviations, which is a bad approach, because it omits the defrost temperatures, which is bad because the cooling unit may be good but the anomoly may be in the defrosting unit/compressor of the cooling unit.


How would we better make an analysis?My EVENTUAL goal would be a ML learning model to predict a failure in the unit, the defrost cycle, the sensor itself etc

----Clarification questions and responses from the lead engineer:----




No training dataset.

Can we assume most of the units have a consistent cyclical nature when functional?
We can assume that most refrigeration units will display roughly cyclical patterns during normal operations.  Failure to maintain periodicity could indicate unusual operating conditions (e.g. someone left a door open or there is no power) or could indicate a problematic unit (needs to be replaced).

Is there an acceptable temperature threshold for each sensor? 
There are three types of environments we monitor, each with its own thresholds:

Ambient:  these are devices monitoring the open air in a store.  As long as it doesn't go above 85 and doesn't freeze, it's okay
Cooler:  these are refrigerators meant to keep things cold but not frozen.  Desired temperature range is roughly 35-45 Fahrenheit
Freezer: these are refrigerators meant to keep things frozen.  Desired temperature range is -55-+5 Fahrenheit

For identifying outliers, how many cycles do you need to see, what is the cooling period?
When I implemented a similar system, for cooling cycles I was comfortable after roughly ten to twenty cycles.  For defrost cycles it usually only took about 5 to be comfortable with a basic range.

What statistical distribution/vantage point is best to analyze the data? Mixed distribution? In fact, with the original model and 3sd, the bands for defrost and cooling unit temperatures are pretty clear, why not just randomly make a 'cut point' to distinguish the two bands and then proceed with finding outliers from mean/sd?
There are outliers in the sense of a distribution and outliers in the sense of readings that go outside an acceptable temperature boundary.  The former tell us about the quality of the refrigeration unit, in the sense of its ability to robustly maintain a well-controlled environment.  The latter adds in factors such as the temperature control that has been set (most home fridges have a dial for users to set how cold their fridge will be, and most commercial devices have a similar controller).  A fridge can maintain a really tight temperature range, and therefore be robust (not have many 'anomalous' readings relative to its distribution), but be set above (or below) the temperature range desired and thus be continually non-compliant.

Given that we need a cooler to maintain temps in a roughly 10 degree F range, the better units are going to be the ones that can generally maintain a roughly 5 degree range, allowing that we can then tune it to the middle of the 10 degree range and  monitor for situations where it strays outside its normal variability and approaches one of the thresholds that really matter.  In ranking coolers, you may want to focus more on developing a score reflecting how tight its distribution is relative to the known range that is acceptable.  As an example, if a cooler can maintain a range of 3 or less, then it gets a top score.  The score starts to go down as the range increases, up to 8 or 9 degrees, but then devices that have a normal range of 10 degrees or more get a minimal score.

Let me know if you want to jump on a call/hangout to talk further, or we can continue over email if you'd prefer and want to discuss further...